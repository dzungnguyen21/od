{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12374497,"sourceType":"datasetVersion","datasetId":7802507},{"sourceId":487494,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":388814,"modelId":407737},{"sourceId":487496,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":388816,"modelId":407739},{"sourceId":487497,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":388817,"modelId":407740}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pip install torch torchvision opencv-python tqdm pycocotools ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset # Import Dataset\nimport torchvision.transforms.functional as F\nfrom PIL import Image\nimport random\n# from pycocotools.coco import COCO # You can remove this import as it's no longer needed\n\nclass CustomYOLODataset(Dataset):\n    def __init__(self, img_folder, label_folder, train=False):\n        self.img_folder = img_folder\n        self.label_folder = label_folder\n        self.train = train\n\n        # Get list of image files. Assuming image and label files have the same base name.\n        self.image_files = [f for f in os.listdir(img_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        self.image_files.sort() # Ensure consistent order for pairing with labels\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.img_folder, img_name)\n        # Create the corresponding label file name (e.g., 'image.jpg' -> 'image.txt')\n        label_name = img_name.rsplit('.', 1)[0] + '.txt'\n        label_path = os.path.join(self.label_folder, label_name)\n\n        img = Image.open(img_path).convert(\"RGB\")\n        image_width, image_height = img.size\n\n        boxes = []\n        labels = []\n        areas = []\n\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                for line in f:\n                    parts = list(map(float, line.strip().split()))\n                    class_id = int(parts[0])\n                    x_center_norm, y_center_norm, width_norm, height_norm = parts[1:]\n\n                    # Convert YOLO format (normalized center_x, center_y, width, height)\n                    # to Pascal VOC format (x_min, y_min, x_max, y_max) in pixel coordinates\n                    x_min = (x_center_norm - width_norm / 2) * image_width\n                    y_min = (y_center_norm - height_norm / 2) * image_height\n                    x_max = (x_center_norm + width_norm / 2) * image_width\n                    y_max = (y_center_norm + height_norm / 2) * image_height\n\n                    # Clamp coordinates to image boundaries to prevent issues\n                    x_min = max(0.0, x_min)\n                    y_min = max(0.0, y_min)\n                    x_max = min(float(image_width), x_max)\n                    y_max = min(float(image_height), y_max)\n\n                    # Only add valid bounding boxes (width and height > 0)\n                    if x_max > x_min and y_max > y_min:\n                        boxes.append([x_min, y_min, x_max, y_max])\n                        labels.append(class_id)\n                        areas.append((x_max - x_min) * (y_max - y_min)) # Calculate area\n\n        if not boxes: # Handle images with no objects or only invalid boxes\n            formatted_target = {\n                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n                \"labels\": torch.zeros(0, dtype=torch.int64),\n                \"image_id\": torch.tensor([idx]), # Use idx as a simple image_id\n                \"area\": torch.zeros(0, dtype=torch.float32),\n                \"iscrowd\": torch.zeros(0, dtype=torch.int64)\n            }\n        else:\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n            areas = torch.as_tensor(areas, dtype=torch.float32)\n            iscrowd = torch.zeros((len(boxes),), dtype=torch.int64) # Assuming no crowd annotations in YOLO txt\n\n            formatted_target = {\n                \"boxes\": boxes,\n                \"labels\": labels,\n                \"image_id\": torch.tensor([idx]), # Use idx as a simple image_id\n                \"area\": areas,\n                \"iscrowd\": iscrowd\n            }\n\n        # Augmentation (Random Horizontal Flip)\n        if self.train and random.random() < 0.5:\n            img = F.hflip(img)\n            if formatted_target[\"boxes\"].numel() > 0: # Only flip boxes if there are any\n                bbox = formatted_target[\"boxes\"]\n                # New x_min = image_width - old x_max\n                # New x_max = image_width - old x_min\n                bbox[:, [0, 2]] = image_width - bbox[:, [2, 0]]\n                formatted_target[\"boxes\"] = bbox\n\n        # Convert PIL image to PyTorch Tensor\n        img = F.to_tensor(img)\n        return img, formatted_target","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_transform(train):\n    transforms = []\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms) if transforms else None\n\n# --- 3. Model Definition ---\ndef get_model(num_classes):\n    # load a model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\", pretrain = True)\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model\n\n# --- 4. Collate Function for Dataloader ---\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport time\nimport json\nimport os\nimport cv2\nfrom torchvision.transforms import functional as F\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef load_model(model_name, weight_path):\n\n    if model_name == 'fasterrcnn':\n        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=92)\n    elif model_name == 'retinanet':\n        model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=92)\n    elif model_name == 'yolo':\n        model = torch.hub.load('ultralytics/yolov5', 'custom', path=weight_path).autoshape()\n        return model.to(device), True\n\n    state_dict = torch.load(weight_path, map_location=device, weights_only=False)\n    model.load_state_dict(state_dict)\n    model.to(device).eval()\n    return model, False\n\ndef run_inference(model, image_paths, is_yolo=False, coco_gt=None):\n    results = []\n    total_time = 0\n    for img_id, img_path in enumerate(tqdm(image_paths)):\n        img = cv2.imread(img_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w = img.shape[:2]\n\n        if is_yolo:\n            start = time.time()\n            results_yolo = model(img_rgb, size=640)\n            end = time.time()\n            total_time += (end - start)\n            for det in results_yolo.xyxy[0]:\n                x1, y1, x2, y2, conf, cls = det.cpu().numpy()\n                results.append({\n                    \"image_id\": img_id + 1,\n                    \"category_id\": int(cls),\n                    \"bbox\": [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n                    \"score\": float(conf)\n                })\n        else:\n            tensor_img = F.to_tensor(img_rgb).unsqueeze(0).to(device)\n            start = time.time()\n            with torch.no_grad():\n                outputs = model(tensor_img)[0]\n            end = time.time()\n            total_time += (end - start)\n            for box, label, score in zip(outputs['boxes'], outputs['labels'], outputs['scores']):\n                x1, y1, x2, y2 = box.tolist()\n                results.append({\n                    \"image_id\": img_id + 1,\n                    \"category_id\": int(label),\n                    \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n                    \"score\": float(score)\n                })\n    avg_time = total_time / len(image_paths)\n    fps = 1 / avg_time\n    print(f\"[INFO] Avg Inference Time: {avg_time:.4f}s | FPS: {fps:.2f}\")\n    return results, avg_time, fps\n\ndef evaluate_coco(preds, ann_file):\n    coco_gt = COCO(ann_file)\n    coco_dt = coco_gt.loadRes(preds)\n    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n    return coco_eval.stats  # mAP@0.5, mAP@0.5:0.95, etc.\n\ndef benchmark(model_name, weight_path, image_dir, ann_file):\n    print(f\"--- Benchmarking {model_name.upper()} ---\")\n    model, is_yolo = load_model(model_name, weight_path)\n    image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))])\n    preds, avg_time, fps = run_inference(model, image_paths, is_yolo)\n    with open(\"predictions.json\", \"w\") as f:\n        json.dump(preds, f)\n    stats = evaluate_coco(\"predictions.json\", ann_file)\n    return {\n        \"model\": model_name,\n        \"mAP@0.5:0.95\": round(stats[0], 3),\n        \"mAP@0.5\": round(stats[1], 3),\n        \"FPS\": round(fps, 2),\n        \"Inference Time (s)\": round(avg_time, 4)\n    }\n\n# def benchmark(model_name, weight_path, image_dir, label_file):\n#     print(f\"--- Benchmarking {model_name.upper()} ---\")\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport cv2\n\ndef convert_yolo_to_coco(yolo_label_dir, image_dir, output_json, categories):\n    images = []\n    annotations = []\n    ann_id = 1\n    image_id = 1\n\n    category_ids = {cat['id']: cat['name'] for cat in categories}\n\n    for filename in sorted(os.listdir(image_dir)):\n        if not filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n            continue\n\n        image_path = os.path.join(image_dir, filename)\n        label_path = os.path.join(yolo_label_dir, os.path.splitext(filename)[0] + \".txt\")\n\n        img = cv2.imread(image_path)\n        if img is None:\n            print(f\"[WARNING] Could not read image: {filename}\")\n            continue\n\n        h, w = img.shape[:2]\n\n        images.append({\n            \"id\": image_id,\n            \"width\": w,\n            \"height\": h,\n            \"file_name\": filename\n        })\n\n        if os.path.exists(label_path):\n            with open(label_path, \"r\") as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) != 5:\n                        continue\n                    class_id, xc, yc, bw, bh = map(float, parts)\n                    x = (xc - bw / 2) * w\n                    y = (yc - bh / 2) * h\n                    width = bw * w\n                    height = bh * h\n\n                    annotations.append({\n                        \"id\": ann_id,\n                        \"image_id\": image_id,\n                        \"category_id\": int(class_id),\n                        \"bbox\": [x, y, width, height],\n                        \"area\": width * height,\n                        \"iscrowd\": 0\n                    })\n                    ann_id += 1\n\n        image_id += 1\n\n    coco_format = {\n        \"info\": {\n            \"description\": \"Converted from YOLO\",\n            \"version\": \"1.0\",\n            \"year\": 2025,\n            \"contributor\": \"\",\n            \"date_created\": \"\"\n        },\n        \"licenses\": [],\n        \"images\": images,\n        \"annotations\": annotations,\n        \"categories\": categories\n    }\n\n    with open(output_json, \"w\") as f:\n        json.dump(coco_format, f, indent=2)\n\n    print(f\"[INFO] COCO annotation JSON saved to: {output_json}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categories = [{\"id\": i, \"name\": f\"class_{i}\"} for i in range(91)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"convert_yolo_to_coco(\n    yolo_label_dir=\"/kaggle/input/coco-minitrain-10k/coco_minitrain_10k/labels/val2017\",\n    image_dir=\"/kaggle/input/coco-minitrain-10k/coco_minitrain_10k/images/val2017\",\n    output_json=\"converted_val2017.json\",\n    categories=categories\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-25T15:19:15.349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_root = '/kaggle/input/coco-minitrain-10k/coco_minitrain_10k/'\nmodels = [\n    ('fasterrcnn', '/kaggle/input/faster-r-cnn/other/default/1/fasterrcnn_best_model.pt'),\n    ('dert', '/kaggle/input/dert/other/default/1/rtdetr_coco_mini.pt'),\n    ('yolo', '/kaggle/input/yolo/other/default/1/yolo11n.pt'),\n]\n\n# image_dir = 'data/val2017/images'\n# ann_file = 'data/val2017/annotations/instances_val2017.json'\nimage_dir = os.path.join(data_root, 'images', 'val2017')   \n# ann_dir = os.path.join(data_root, 'labels', 'val2017')\nann_file='/kaggle/working/converted_val2017.json'\n\nall_results = []\nfor model_name, weight in models:\n    result = benchmark(model_name, weight, image_dir, ann_file)\n    all_results.append(result)\n\nprint(\"\\n--- Summary Benchmark ---\")\nfor r in all_results:\n    print(r)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-25T15:19:15.349Z"}},"outputs":[],"execution_count":null}]}